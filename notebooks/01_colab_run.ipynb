{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "setup-cell-universal",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running in Google Colab\n",
      "Cloning into 'reliable-echo-segment'...\n",
      "remote: Enumerating objects: 38, done.\u001b[K\n",
      "remote: Counting objects: 100% (38/38), done.\u001b[K\n",
      "remote: Compressing objects: 100% (31/31), done.\u001b[K\n",
      "remote: Total 38 (delta 3), reused 35 (delta 3), pack-reused 0 (from 0)\u001b[K\n",
      "Receiving objects: 100% (38/38), 18.85 KiB | 18.85 MiB/s, done.\n",
      "Resolving deltas: 100% (3/3), done.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "try:\n",
    "    import google.colab\n",
    "    IN_COLAB = True\n",
    "except ImportError:\n",
    "    IN_COLAB = False\n",
    "\n",
    "if IN_COLAB:\n",
    "    print(\"Running in Google Colab\")\n",
    "    if not os.path.exists(\"reliable-echo-segment\"):\n",
    "        !git clone https://github.com/khenm/reliable-echo-segment.git\n",
    "    os.chdir(\"/content/reliable-echo-segment\")\n",
    "    sys.path.append(\"/content/reliable-echo-segment\")\n",
    "    %pip install -r requirements.txt -qq\n",
    "else:\n",
    "    print(\"Running locally\")\n",
    "    # Move up to project root if we are in notebooks directory\n",
    "    if os.getcwd().endswith(\"notebooks\"):\n",
    "        os.chdir(\"..\")\n",
    "    \n",
    "    project_root = os.getcwd()\n",
    "    if project_root not in sys.path:\n",
    "        sys.path.append(project_root)\n",
    "    print(f\"Working directory: {project_root}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "run-cell",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:torchao.kernel.intmm:Warning: Detected no triton, on systems without Triton certain kernels will not work\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-12-19 04:14:50,418 - CAMUS - INFO - Starting notebook run...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:CAMUS:Starting notebook run...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-12-19 04:14:50,427 - CAMUS - INFO - Using device: cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:CAMUS:Using device: cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-12-19 04:14:50,429 - CAMUS - INFO - Initializing DataLoaders...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:CAMUS:Initializing DataLoaders...\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/Users/khenm/Materials/Datasets/CAMUS_public/database_split/subgroup_training.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipython-input-298872649.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;31m# Data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Initializing DataLoaders...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m \u001b[0mloaders\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_dataloaders\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcfg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;31m# Model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/content/reliable-echo-segment/src/dataset.py\u001b[0m in \u001b[0;36mget_dataloaders\u001b[0;34m(cfg)\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0mnii_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcfg\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'data'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'nifti_dir'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m     \u001b[0mids_tr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_read_ids\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msplit_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"subgroup_training.txt\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m     \u001b[0mids_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_read_ids\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msplit_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"subgroup_validation.txt\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0mids_ts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_read_ids\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msplit_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"subgroup_testing.txt\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/content/reliable-echo-segment/src/dataset.py\u001b[0m in \u001b[0;36m_read_ids\u001b[0;34m(txt_path)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_read_ids\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtxt_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtxt_path\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/Users/khenm/Materials/Datasets/CAMUS_public/database_split/subgroup_training.txt'"
     ]
    }
   ],
   "source": [
    "import yaml\n",
    "import torch\n",
    "import logging\n",
    "import os\n",
    "from src.utils.util_ import seed_everything, get_device\n",
    "from src.dataset import get_dataloaders\n",
    "from src.models.model import get_model\n",
    "from src.trainer import Trainer\n",
    "from src.utils.logging import get_logger\n",
    "\n",
    "# 1. Create output directories\n",
    "os.makedirs(\"checkpoints\", exist_ok=True)\n",
    "os.makedirs(\"runs\", exist_ok=True)\n",
    "\n",
    "# 2. Config Setup\n",
    "config_path = \"configs/config.yaml\"\n",
    "with open(config_path, 'r') as f:\n",
    "    cfg = yaml.safe_load(f)\n",
    "\n",
    "# Override paths to use new directories\n",
    "cfg['training']['ckpt_save_path'] = os.path.join(\"checkpoints\", \"best_camus.pt\")\n",
    "cfg['training']['test_metrics_csv'] = os.path.join(\"runs\", \"camus_test_metrics.csv\")\n",
    "\n",
    "# 3. Logging Setup\n",
    "# Add file handler to save logs to runs/train.log\n",
    "logger = get_logger()\n",
    "file_handler = logging.FileHandler(os.path.join(\"runs\", \"train.log\"))\n",
    "formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "file_handler.setFormatter(formatter)\n",
    "logger.addHandler(file_handler)\n",
    "\n",
    "logger.info(\"Starting notebook run...\")\n",
    "\n",
    "# 4. Initialization\n",
    "seed_everything(cfg['training']['seed'])\n",
    "device = get_device()\n",
    "logger.info(f\"Using device: {device}\")\n",
    "\n",
    "# Data\n",
    "logger.info(\"Initializing DataLoaders...\")\n",
    "loaders = get_dataloaders(cfg)\n",
    "\n",
    "# Model\n",
    "logger.info(\"Initializing Model...\")\n",
    "model = get_model(cfg, device)\n",
    "logger.info(f\"Model parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "\n",
    "# 5. Training\n",
    "trainer = Trainer(model, loaders, cfg, device)\n",
    "trainer.train()\n",
    "\n",
    "# 6. Evaluation\n",
    "logger.info(\"Starting Evaluation...\")\n",
    "trainer.evaluate_test()\n",
    "logger.info(\"Run finished.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
